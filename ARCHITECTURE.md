## Agent Framework: High-Level Architectural Overview

The agent framework is designed with a modular architecture to facilitate the creation, management, and execution of agent-based workflows. The core components and their interactions are detailed below:

### 1. Initialization and Configuration (`bootstrap.py`, `config.py`, `logger.py`, `paths.py`)

-   `bootstrap.py`: Orchestrates the initial setup of the entire system. It ensures directories are created (`paths.py`), logging is configured (`logger.py`), configuration settings are loaded (`config.py`), LLM clients are initialized (`llms/client.py`), the agent registry is populated (`agents/registry.py`), and the session manager is started (`session_manager.py`).
-   `config.py`: Manages application settings, including API keys, endpoints, and logging levels, typically loaded from environment variables or a `.env` file.
-   `logger.py`: Sets up a comprehensive logging system with different handlers for console output (colored) and file storage (standard and JSON formats).
-   `paths.py`: Provides utility functions for path manipulation, including locating the project root directory and ensuring directories exist.

### 2. Agent Management (`agents/` directory, `agent_loader.py`, `agents/registry.py`, `agents/factory.py`)

-   `agent_loader.py`: Responsible for discovering and parsing agent specifications (e.g., from YAML or JSON files) into structured `AgentSpec` models.
-   `agents/registry.py`: Acts as a central registry for both agent specifications and their corresponding Python class implementations. It loads specs via `agent_loader` and registers concrete agent classes (like `DummyAgent`).
-   `agents/factory.py`: A factory pattern implementation used to create instances of agent classes. It queries the `AgentRegistry` to find the appropriate agent class based on a given name.
-   `agents/base.py`: Defines the abstract base class (`Agent`) that all concrete agents must inherit from, specifying the core `run` method signature.

### 3. Workflow Execution (`orchestrator.py`, `task_manager.py`, `task_dependencies.py`, `workflow/` directory)

-   `orchestrator.py`: The heart of the execution engine. It receives a list of tasks, uses `task_dependencies` to sort them topologically, and then dispatches each task to an agent created by the `AgentFactory`. It manages task status updates and error handling during execution.
-   `task_manager.py`: Implements a `TaskQueue` to hold tasks, manage their states (pending, in_progress, completed, failed), and provide them to the `Orchestrator` in the correct order.
-   `task_dependencies.py`: Contains logic for validating task dependencies, including detecting circular dependencies and performing topological sorting to determine the correct execution order.
-   `workflow/planner.py`: Responsible for generating and validating the execution plan (the ordered list of tasks).
-   `workflow/state.py`: Manages the overall state of the workflow execution (e.g., RUNNING, COMPLETED, FAILED) using a state machine pattern.

### 4. Session and Artifact Management (`session_manager.py`, `artifacts.py`)

-   `session_manager.py`: Manages the lifecycle of an individual workflow execution session. It tracks session start/end times, status, logs, and associated artifacts. It coordinates with `artifact_manager` for storage.
-   `artifacts.py`: Handles the persistence and retrieval of artifacts generated by agents. It defines a directory structure for storing these artifacts, often organized by session ID.

### 5. LLM Integration (`llms/` directory)

-   `llms/client.py`: Initializes and provides access to various LLM providers configured in `config.py` (e.g., Gemini, Ollama, Kimi, Mistral).
-   `llms/provider.py`: Defines the abstract interface (`LLMProvider`) that all specific LLM integrations must adhere to, primarily the `generate` method.
-   `llms/gemini.py`, `llms/ollama.py`, etc.: Concrete implementations of `LLMProvider` for different LLM services.

### 6. Supporting Utilities (`context.py`, `error_handling.py`, `file_io.py`, `prompt_manager.py`, `prompt_templates.py`, `response_parser.py`, `models.py`)

-   `context.py`: Manages the execution context, including environment variables and runtime flags.
-   `error_handling.py`: Provides decorators for exception handling and retry logic.
-   `file_io.py`: Basic file read/write utilities.
-   `prompt_manager.py`: Loads and provides access to prompt templates.
-   `prompt_templates.py`: Handles rendering of prompt templates, potentially using Jinja2.
-   `response_parser.py`: Parses agent responses, attempting to deserialize JSON or falling back to plain text.
-   `models.py`: Defines the core data structures (Pydantic models) used throughout the system (e.g., `AgentSpec`, `TaskSpec`, `Artifact`, `AgentResponse`).

### Key Interaction Flows:

1.  **Workflow Execution**: `main.py` -> `bootstrap_system()` -> `cli.py` (parses command) -> `workflow_loader.load_workflow_from_file()` -> `Orchestrator.run_workflow()` -> `task_manager.task_queue` & `agents/factory.AgentFactory` (using `agents/registry.AgentRegistry`) -> Agent execution -> `session_manager.add_artifact()` / `session_manager.add_log_entry()` -> `workflow/state.WorkflowStateMachine` updates.
2.  **Agent Loading**: `bootstrap.py` calls `agents/registry.AgentRegistry._load_initial_agent_specs()` which uses `agent_loader.load_agent_specs()`.
3.  **LLM Interaction**: `Orchestrator` or specific agents might use `llms.client.llm_client.get_provider()` to obtain an LLM provider and call its `generate` method.

This architecture emphasizes separation of concerns, making the framework extensible and maintainable.

## `models.py` - Core Data Structures

The `src/models.py` module defines the fundamental data structures used throughout the agent framework. These structures, implemented using Pydantic models, ensure data consistency and validation across different components.

### Key Models:

-   `AgentSpec`: Represents the specification for an agent, including its `name`, `role`, and a brief `description`.
-   `TaskSpec`: Defines a single task within a workflow. It includes a unique `id`, `name`, `description`, the `agent_name` responsible for executing it, any `input_data` required, and a list of `dependencies` (other task IDs it relies on).
-   `Artifact`: Represents a piece of data produced or consumed by an agent. It has a `name`, `type` (e.g., 'text/plain', 'application/json'), and the actual `data`.
-   `Session`: Encapsulates the state and results of a single workflow execution run. It includes a unique `id`, `start_time`, optional `end_time`, overall `status`, a list of `logs`, and a list of generated `artifacts`.
-   `ExecutionContext`: Holds the runtime context for an operation, such as the `session_id`, environment variables (`env_vars`), and other runtime flags (`runtime_flags`).
-   `AgentResponse`: Represents the output returned by an agent after executing a task. It includes a `status` (e.g., 'completed', 'failed'), any processed `output` data, and a list of `artifacts` produced.

These models provide a standardized way to represent and exchange information between different parts of the agent framework, ensuring type safety and clarity.

# Configuration (`config.py`)

The `config.py` module is responsible for managing the application's configuration settings. It utilizes the `pydantic-settings` library to load settings from environment variables and a `.env` file, providing a robust and flexible way to configure the agent framework.

## Settings Class

The core of this module is the `Settings` class, which inherits from `pydantic_settings.BaseSettings`. This class defines the structure and types of all configuration parameters.

### Key Configuration Parameters:

-   `APP_NAME`: (str) The name of the application. Defaults to "AgentFramework".
-   `LOG_LEVEL`: (str) The minimum severity level for log messages to be processed. Defaults to "INFO". Supported levels include DEBUG, INFO, WARNING, ERROR, CRITICAL.
-   `PROMPTS_DIR`: (str) The directory where prompt templates are stored. Defaults to "prompts".
-   `ARTIFACTS_DIR`: (str) The directory where generated artifacts are stored. Defaults to "artifacts".

### LLM Settings:

These settings configure the connection details and API keys for various Large Language Model (LLM) providers.

-   `GEMINI_API_KEY`: (Optional[str]) API key for the Gemini LLM. If not provided, the Gemini provider may not be initialized.
-   `GEMINI_ENDPOINT`: (str) The API endpoint URL for Gemini. Defaults to "https://api.gemini.com/v1".
-   `OLLAMA_HOST`: (str) The host address for the Ollama service. Defaults to "http://localhost:11434".
-   `KIMI_API_KEY`: (Optional[str]) API key for the Kimi LLM. If not provided, the Kimi provider may not be initialized.
-   `KIMI_ENDPOINT`: (str) The API endpoint URL for Kimi. Defaults to "https://api.kimi.ai/v1".
-   `MISTRAL_API_KEY`: (Optional[str]) API key for the Mistral LLM. If not provided, the Mistral provider may not be initialized.
-   `MISTRAL_ENDPOINT`: (str) The API endpoint URL for Mistral. Defaults to "https://api.mistral.ai/v1".
-   `ACTIVE_LLM_PROVIDERS`: (str) A comma-separated string specifying which LLM providers should be activated. Example: "gemini,ollama".

## Loading Configuration

Configuration is loaded automatically when the `Settings` class is instantiated, typically during system bootstrap. It prioritizes environment variables over values defined in a `.env` file, which in turn overrides the default values set in the class.

```json
{
  "APP_NAME": "AgentFramework",
  "LOG_LEVEL": "INFO",
  "PROMPTS_DIR": "prompts",
  "ARTIFACTS_DIR": "artifacts",
  "GEMINI_API_KEY": null,
  "GEMINI_ENDPOINT": "https://api.gemini.com/v1",
  "OLLAMA_HOST": "http://localhost:11434",
  "KIMI_API_KEY": null,
  "KIMI_ENDPOINT": "https://api.kimi.ai/v1",
  "MISTRAL_API_KEY": null,
  "MISTRAL_ENDPOINT": "https://api.mistral.ai/v1",
  "ACTIVE_LLM_PROVIDERS": "gemini"
}
```

*Note: The actual values for API keys and endpoints will depend on your environment setup.*

## Usage

An instance of the `Settings` class, named `settings`, is exported and can be imported and used throughout the application to access configuration values.

```python
# Example usage in another module:
from src.config import settings

print(f"Application Name: {settings.APP_NAME}")
if settings.GEMINI_API_KEY:
    print("Gemini API key is configured.")
```

## `logger.py`: Logging Setup and Configuration

The `logger.py` module is responsible for establishing and configuring the application's logging system. It ensures that logs are captured, formatted, and directed to appropriate destinations, such as the console and log files.

### Key Components:

-   `ColoredFormatter`: Extends `logging.Formatter` to add color-coding to log messages based on their severity level (e.g., red for ERROR, yellow for WARNING, green for INFO). This enhances readability of console output. It overrides the `format` method to prepend ANSI escape codes for colors before the standard log message and reset them afterward.
-   `JsonFormatter`: Extends `logging.Formatter` to format log records as JSON objects. This is useful for structured logging, enabling easier parsing and analysis by log aggregation systems. It overrides the `format` method to create a dictionary representation of the log record and then serializes it into a JSON string.
-   `setup_logging()` Function: The main function that orchestrates the entire logging setup. It retrieves the desired log level from `settings.LOG_LEVEL`, ensures the log directory (`logs/`) exists, creates and configures handlers for console output (using `ColoredFormatter`), and for file output (`app.log` with standard format and `app.json` with `JsonFormatter`). All configured handlers are added to the root logger.

### How it's Used:

The `setup_logging()` function is called early in the application's lifecycle, typically within `bootstrap.py`, to ensure that all subsequent logging messages from any module are captured and processed according to this configuration.

## Module: src/artifacts.py

### Purpose

The `artifacts.py` module is responsible for managing the lifecycle of artifacts generated during workflow execution. This includes storing artifacts to disk, retrieving them when needed, and potentially providing mechanisms for contextualization based on previously generated artifacts.

### Core Component: `ArtifactManager`

The central class in this module is `ArtifactManager`. It handles the file-based persistence of artifacts.

#### Initialization (`__init__`)

-   The `ArtifactManager` is initialized with an optional `artifact_dir` argument, which defaults to `'artifacts'` relative to the project root.
-   It ensures that this directory exists upon instantiation, creating it if necessary.

#### Key Methods

-   `store_artifact(artifact: Artifact, session_id: str)`:
    -   Takes an `Artifact` object and a `session_id` as input.
    -   Constructs a file path for the artifact, typically organized within a subdirectory named after the `session_id` inside the main artifact directory (e.g., `artifacts/<session_id>/<artifact_name>`).
    -   Ensures the target directory for the artifact exists.
    -   Writes the artifact's data to the specified file path. Currently, it converts the data to a string using `str(artifact.data)` and uses `write_file`.

-   `retrieve_artifact(name: str, session_id: str) -> Optional[Artifact]`:
    -   Takes the artifact's `name` and `session_id` to locate the file on disk.
    -   If the artifact file exists, it reads the content using `read_file`.
    -   It then constructs and returns an `Artifact` object.
    -   Returns `None` if the artifact file is not found.

-   `get_contextual_artifacts(session_id: str) -> Dict[str, Artifact]`:
    -   This method is intended to provide artifacts that can be used for contextualizing future agent tasks.
    -   Currently, it's a placeholder that returns an empty dictionary.

### Instance

A singleton instance, `artifact_manager`, is created and exported for use throughout the application.

### Interactions

-   The `ArtifactManager` is typically used by the `SessionManager` to store and retrieve artifacts associated with a specific session.
-   It relies on `src.file_io` for underlying file read/write operations and `src.paths` for directory management.
-   The `Artifact` model from `src.models` defines the structure of artifacts being managed.

## Documentation for `session_manager.py`

### Purpose

The `session_manager.py` module is responsible for managing the lifecycle of individual workflow execution sessions. It tracks session start and end times, maintains the session's status, and persists logs and artifacts generated during the session.

### Core Components

-   `SessionManager` Class: This is the primary class within the module. It handles:
    -   Session Lifecycle: Starting new sessions with unique IDs and ending them, updating their status accordingly.
    -   State Management: Interacting with the `WorkflowStateMachine` to synchronize session status with the overall workflow state.
    -   Log Persistence: Appending log entries to the current session's log.
    -   Artifact Management: Associating artifacts with the current session and coordinating with the `ArtifactManager` to store them persistently.

-   `start_session()` Method: Initializes a new session with a unique UUID, sets the start time, initializes the status to `WorkflowState.INIT`, and associates it with the `workflow_state_machine`. It returns the newly created `Session` object.

-   `end_session(status: WorkflowState)` Method: Marks the current session as ended by setting the `end_time` and updating the session's status to the provided `WorkflowState`. It also logs the session end and resets the `_current_session` attribute.

-   `add_log_entry(entry: str)` Method: Appends a given log string to the `logs` list of the current session.

-   `add_artifact(artifact: Artifact)` Method: Adds an `Artifact` object to the session's artifact list and calls `artifact_manager.store_artifact` to save the artifact to disk, organizing it by session ID.

-   `get_current_session()` Method: Returns the currently active `Session` object.

### Interactions

-   `WorkflowStateMachine`: The `SessionManager` updates the state machine when starting and ending sessions, ensuring consistency between the session's status and the overall workflow state.
-   `ArtifactManager`: The `SessionManager` delegates the actual storage of artifacts to the `ArtifactManager`.
-   `uuid`: Used to generate unique identifiers for each session.
-   `datetime`: Used to timestamp session start and end times.
-   `logging`: Used for logging information related to session management activities.

### Session Data Model (`src.models.Session`)

The `Session` model, defined in `src/models.py`, represents the data structure for a session.

### Usage Example (Conceptual)

```python
# In orchestrator.py or main execution flow:

session = session_manager.start_session()
logger.info(f"Starting workflow for session: {session.id}")

# During task execution:
response = agent.run(task)
if response.artifacts:
    for artifact in response.artifacts:
        session_manager.add_artifact(artifact)
        logger.info(f"Stored artifact: {artifact.name}")

session_manager.add_log_entry(f"Task {task.id} completed.")

# At the end of the workflow:
session_manager.end_session(status=WorkflowState.COMPLETED)
```

This module ensures that each run of the agent framework is properly tracked and its outputs are managed within the context of a specific session.

## LLM Integration Documentation

The agent framework supports integration with various Large Language Models (LLMs) through a modular provider-based system. This allows for flexibility in choosing and configuring different LLM services.

### Core Components:

-   `llms/provider.py`: This abstract base class defines the contract for all LLM providers. Any concrete LLM provider must implement the `generate(prompt: str) -> str` method, which is responsible for sending a prompt to the LLM and returning its text response.
-   `llms/client.py`: The `LLMClient` class acts as a central manager for LLM providers. It initializes available providers based on the `ACTIVE_LLM_PROVIDERS` setting in `config.py`. It dynamically instantiates specific provider classes (e.g., `GeminiLLMProvider`, `OllamaLLMProvider`) if their corresponding API keys or configurations are present. The `get_provider(name: str)` method allows other parts of the system to retrieve an initialized LLM provider instance by its name (e.g., 'gemini', 'ollama').
-   Specific Provider Implementations (`llms/gemini.py`, `llms/ollama.py`, `llms/kimi.py`, `llms/mistral.py`): These files contain concrete implementations of the `LLMProvider` interface for different LLM services. Each provider's `generate` method contains placeholder logic for making the actual API calls.
-   `config.py`: This file centralizes all configuration, including API keys, endpoints, and the list of active LLM providers (`ACTIVE_LLM_PROVIDERS`).
-   `bootstrap.py`: During system initialization, `bootstrap_system()` calls `llm_client._initialize_providers()` to ensure that the configured LLM clients are ready for use before the main application logic begins.

### How LLMs are Used:

Agents within the framework can leverage LLMs by obtaining an LLM provider instance from the `LLMClient`, calling the `generate` method with a specific prompt, and processing the returned text response.

## Agent Package Documentation

This document details the `agents` package, which is central to the framework's ability to define, manage, and instantiate agent functionalities.

### 1. Agent Architecture (`src/agents/base.py`)

The foundation of the agent system is the abstract base class `Agent` located in `src/agents/base.py`. All concrete agent implementations must inherit from this class and implement the abstract `run` method.

-   `Agent(ABC)`:
    -   `__init__(self, name: str)`: Initializes the agent with a unique name.
    -   `run(self, task: TaskSpec) -> AgentResponse`: An abstract method that must be implemented by subclasses. It takes a `TaskSpec` as input, processes it, and returns an `AgentResponse` detailing the outcome, any generated output data, and artifacts.

### 2. Agent Loading and Specification (`src/agent_loader.py`)

Agent behavior and configuration are defined in specification files. The `src/agent_loader.py` module is responsible for reading these specifications and converting them into structured `AgentSpec` models.

-   `load_agent_specs(spec_dir: str = "agents/specs") -> List[AgentSpec]`:
    -   Scans the specified directory (defaults to `agents/specs`).
    -   Parses files ending in `.yaml`, `.yml`, or `.json`.
    -   Validates the content against the `AgentSpec` model.
    -   Returns a list of `AgentSpec` objects.
    -   Handles file reading errors and parsing errors gracefully, logging warnings or errors.

### 3. Agent Registry (`src/agents/registry.py`)

The `AgentRegistry` acts as a central hub for managing both agent specifications and their corresponding Python class implementations.

-   `AgentRegistry`:
    -   `__init__(self)`: Initializes the registry by loading agent specifications using `agent_loader.load_agent_specs()` and registering known concrete agent classes (e.g., `DummyAgent`).
    -   `register_agent_class(self, agent_class: Type[Agent])`: Registers a concrete agent class, mapping its class name to the class itself.
    -   `get_agent_class(self, name: str) -> Optional[Type[Agent]]`: Retrieves an agent class by its registered name.
    -   `get_agent_spec(self, name: str) -> Optional[AgentSpec]`: Retrieves an agent specification by its name.
    -   `list_agent_specs(self) -> List[AgentSpec]`: Returns all loaded agent specifications.

### 4. Agent Factory (`src/agents/factory.py`)

The `AgentFactory` provides a mechanism for creating agent instances based on their registered names.

-   `AgentFactory`:
    -   `create_agent(self, agent_name: str) -> Agent`:
        -   Looks up the `AgentSpec` in the `AgentRegistry`.
        -   Retrieves the corresponding agent class from the registry.
        -   Instantiates and returns the agent object.
        -   Raises `ValueError` if the agent specification or class is not found.

### 5. Dummy Agent Example (`src/agents/dummy_agent.py`)

Provides a basic implementation of the `Agent` base class for testing and demonstration purposes.

-   `DummyAgent(Agent)`:
    -   Inherits from `Agent`.
    -   Implements the `run` method to simulate task processing, including potential failure conditions and artifact generation.
    -   Logs its actions during initialization and task execution.

### Summary of Agent Loading and Interaction:

1.  **Initialization**: During system bootstrap (`bootstrap.py`), the `AgentRegistry` is populated with agent specifications loaded from files (`agent_loader.py`) and known concrete agent classes are registered (`registry.py`).
2.  **Instantiation**: When the `Orchestrator` needs to execute a task, it uses the `AgentFactory` to create an agent instance.
3.  **Execution**: The `AgentFactory` queries the `AgentRegistry` to find the correct agent class based on the `agent_name` specified in the `TaskSpec` and instantiates it.
4.  **Task Processing**: The instantiated agent's `run` method is called with the `TaskSpec`, returning an `AgentResponse`.

This modular approach allows for easy addition of new agents by simply defining their specifications and implementing the `Agent` base class.

### Module: src/prompt_manager.py

The `PromptManager` class is responsible for handling the loading, storage, and retrieval of prompt templates within the agent framework. It ensures that prompts are readily available for use by agents during their operations.

### Core Functionality:

-   **Initialization and Loading**:
    -   When a `PromptManager` instance is created, it initializes a directory for prompt storage (defaulting to `settings.PROMPTS_DIR` or a specified path).
    -   It automatically loads all `.txt` files from this directory. Each filename (without the extension) is treated as a prompt name, and the file content becomes the prompt's template.
    -   Loaded prompts are stored in an in-memory dictionary (`self.prompts`) mapping prompt names to their content.

-   **Prompt Retrieval**:
    -   The `get_prompt(name: str)` method allows any part of the system to retrieve the content of a specific prompt template by its name.
    -   If a prompt with the given name is not found, it returns an empty string.

-   **Prompt Updates**:
    -   The `update_prompt(name: str, new_content: str)` method enables dynamic updating of prompt templates in memory.

### Usage:

-   The `PromptManager` is typically instantiated once during system bootstrap (`bootstrap.py`) and made available globally.
-   Other modules or agents can then access prompt content through the singleton instance.

### Key Components:

-   `prompt_dir`: The directory where prompt template files (`.txt`) are stored.
-   `prompts`: A dictionary (`Dict[str, str]`) holding the loaded prompt templates in memory.
-   `load_prompts()`: Method to scan the `prompt_dir` and populate the `prompts` dictionary.
-   `get_prompt(name)`: Retrieves a prompt by name.
-   `update_prompt(name, content)`: Updates a prompt in memory.

This module ensures a centralized and efficient way to manage prompt templates.

## `src/bootstrap.py` Module Documentation

### Purpose

The `bootstrap.py` module is responsible for initializing and configuring the entire agent framework system upon application startup. It orchestrates a sequence of setup operations to ensure all necessary components are ready before the main application logic begins execution.

### Key Functions

-   `bootstrap_system()`: This is the primary function within the module. It executes a series of steps in a defined order:
    1.  **Configuration Loading**: Ensures application settings are loaded (implicitly via `src.config.settings`).
    2.  **Path Management**: Verifies and creates essential directories such as `logs`, `artifacts`, `prompts`, and `agents/specs` using utilities from `src.paths`.
    3.  **Logging Setup**: Configures the application-wide logging system by calling `src.logger.setup_logging()`.
    4.  **LLM Client Initialization**: Initializes the Large Language Model client, preparing it to interact with configured LLM providers by calling `src.llms.client.llm_client._initialize_providers()`.
    5.  **Agent Registry Population**: Loads agent specifications from their defined locations into the agent registry using `src.agents.registry.agent_registry._load_initial_agent_specs()`.
    6.  **Session Management Initialization**: Starts the initial session management process via `src.session_manager.session_manager.start_session()`.

### Role in System Initialization

`bootstrap.py` acts as the entry point for system setup. It ensures that foundational elements like configuration, logging, LLM clients, agent definitions, and session management are correctly initialized and configured. This module guarantees that the framework is in a stable and ready state to process user requests or execute workflows.

## Workflow Package Documentation

The `workflow` package is central to managing the execution flow of agent tasks. It comprises two key components: the `planner` and the `state`.

### 1. `workflow/planner.py`

-   **Purpose**: This module is responsible for generating and validating the execution plan for a workflow. It ensures that tasks can be executed in a logical order, respecting their dependencies.
-   **Key Class**: `WorkflowPlanner`
    -   `generate_plan(tasks: List[TaskSpec]) -> List[TaskSpec]`: Takes a list of `TaskSpec` objects and returns a topologically sorted list, representing the execution order. It raises a `ValueError` if circular dependencies are detected among the tasks.
    -   `validate_plan(plan: List[TaskSpec]) -> bool`: Validates an existing execution plan.

### 2. `workflow/state.py`

-   **Purpose**: This module manages the overall state of the workflow execution using a state machine pattern. It tracks whether the workflow is initializing, running, completed, or has failed.
-   **Key Components**:
    -   `WorkflowState` (Enum): Defines the possible states a workflow can be in: `INIT`, `RUNNING`, `COMPLETED`, `FAILED`.
    -   `WorkflowStateMachine`: The class that implements the state machine logic.
        -   `__init__(initial_state: WorkflowState = WorkflowState.INIT)`: Initializes the state machine with a starting state.
        -   `set_session(session: Session)`: Associates the state machine with a `Session` object, allowing it to update the session's status.
        -   `transition_to(new_state: WorkflowState)`: Changes the current state of the workflow to the `new_state`. It also updates the associated session's status if one is set.
        -   `get_state() -> WorkflowState`: Returns the current state of the workflow.

### Workflow Management Summary

The `workflow` package provides the essential logic for defining the order of operations (`planner`) and tracking the progress of an agent-based workflow (`state`). The `WorkflowPlanner` ensures tasks are executable, while the `WorkflowStateMachine` provides visibility into the workflow's overall status.

## Orchestrator Module Documentation (`orchestrator.py`)

The `Orchestrator` class is the central component responsible for managing and executing agent-based workflows. It orchestrates the flow of tasks, dispatches them to appropriate agents, and handles the overall workflow state.

### Core Functionality

-   **Workflow Execution (`run_workflow`)**: This method initiates and manages the entire workflow process. It takes a list of `TaskSpec` objects as input, validates and sorts them based on their dependencies using `task_dependencies`, and then processes them sequentially.
-   **Task Dispatching**: For each task, the `Orchestrator` uses the `AgentFactory` to create an instance of the specified agent. The `AgentFactory` is responsible for looking up and instantiating the correct agent class based on the `agent_name` specified in the `TaskSpec`. It then calls the agent's `run` method with the task details.
-   **State Management**: It interacts with the `WorkflowStateMachine` to transition the workflow through different states (e.g., RUNNING, COMPLETED, FAILED). It also coordinates with the `SessionManager` to log events and store artifacts generated during the workflow execution.
-   **Error Handling**: The `Orchestrator` includes robust error handling to catch exceptions during task execution or dependency resolution. Specifically, exceptions caught during agent execution lead to the task's status being marked as 'failed' and the workflow state transitioning to `FAILED`, and execution is halted.

### Key Interactions

-   `AgentFactory`: Used to instantiate agent objects based on task specifications.
-   `task_queue` (from `task_manager.py`): Tasks are added to this queue after being sorted by dependencies. The orchestrator retrieves tasks from this queue.
-   `task_dependencies`: Utilized for validating task dependencies, detecting cycles, and performing topological sorting to ensure tasks are executed in the correct order.
-   `session_manager`: Records logs and artifacts associated with the current workflow session.
-   `workflow_state_machine`: Manages and updates the overall state of the workflow execution.

### Workflow Lifecycle

1.  **Initialization**: The `run_workflow` method begins by setting the workflow state to `RUNNING` and populating the `task_queue` with topologically sorted tasks.
2.  **Task Processing Loop**: The orchestrator continuously retrieves tasks from the `task_queue`.
3.  **Agent Invocation**: For each task, an agent is created and its `run` method is called.
4.  **Response Handling**: The `AgentResponse` is processed. Task status is updated, and any generated artifacts are stored via the `session_manager`.
5.  **Failure Handling**: If an agent reports failure or an exception occurs, the workflow state is set to `FAILED`, and the orchestration loop is terminated.
6.  **Completion**: If all tasks are processed successfully, the workflow state is set to `COMPLETED`.
7.  **Session Termination**: Finally, the `session_manager` is called to end the session with the determined final status.

## Task Manager (`task_manager.py`)

The `task_manager.py` module is responsible for managing the queue of tasks to be executed within the agent framework. It provides a centralized mechanism for adding tasks, retrieving the next available task, and updating the status of tasks as they progress through the workflow.

### Core Component: `TaskQueue`

The primary component in this module is the `TaskQueue` class. It utilizes a `deque` (double-ended queue) from Python's `collections` module for efficient addition and removal of tasks from both ends, although in this implementation, tasks are processed in a First-In, First-Out (FIFO) manner.

#### Key Methods:

-   `add_task(self, task: TaskSpec)`:
    -   Appends a given `TaskSpec` object to the internal queue.
    -   Initializes the task's status to `'pending'` in the internal status dictionary.

-   `get_next_task(self) -> Optional[TaskSpec]`:
    -   Removes and returns the task at the front of the queue (the next task to be processed).
    -   If the queue is empty, it returns `None`.
    -   Updates the status of the retrieved task to `'in_progress'`.

-   `update_task_status(self, task_id: str, status: str)`:
    -   Updates the status of a specific task, identified by its `task_id`, in the internal status dictionary.
    -   This is crucial for tracking the lifecycle of a task (e.g., from `'in_progress'` to `'completed'` or `'failed'`).

-   `get_task_status(self, task_id: str) -> Optional[str]`:
    -   Retrieves and returns the current status of a task based on its `task_id`.

### Usage within the Framework

The `TaskQueue` instance (`task_queue`) is typically used by the `Orchestrator`. The `Orchestrator` populates the queue with tasks (often after they have been topologically sorted to respect dependencies) and then repeatedly calls `get_next_task()` to fetch tasks for execution. As agents complete their work, the `Orchestrator` uses `update_task_status()` to reflect the outcome in the `TaskQueue`.

This module ensures that task execution order is managed correctly and that the state of each task is tracked throughout the workflow lifecycle.
